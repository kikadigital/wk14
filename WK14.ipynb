{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 14\n",
        "\n",
        "Scikit-Learn and regression modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024S-R/9103-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/DM-GY-9103-2024S-R/9103-utils/raw/main/src/io_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from data_utils import MinMaxScaler, StandardScaler\n",
        "from data_utils import PolynomialFeatures\n",
        "from data_utils import LinearRegression, RandomForestClassifier\n",
        "from data_utils import KMeansClustering, GaussianClustering, SpectralClustering\n",
        "from data_utils import regression_error\n",
        "from io_utils import object_from_json_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKGt8bzScNA"
      },
      "source": [
        "## Regression\n",
        "\n",
        "Regression, or Regression Analysis, is a set of statistical processes for estimating the relationship between a dependent variable (sometimes called the 'outcome', 'response' or 'label') and one or more independent variables (called 'features', 'dimensions' or 'columns').\n",
        "\n",
        "For example, let's say we have the following data about people's wages and years of experience:\n",
        "\n",
        "<img src=\"./imgs/wages-exp.png\" width=\"620px\"/>\n",
        "\n",
        "We could use regression to calculate how the values for wages are affected by years of experience in our dataset, and then create a function to more generally estimate the relation between wages and experience:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-fit.png\" width=\"620px\"/>\n",
        "\n",
        "We could now estimate wages for values of years of experience that we didn't have measurements for.\n",
        "\n",
        "This is an estimate, but the more points we use and the more features we have in our dataset the better the regression results will be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up Regression\n",
        "\n",
        "For a simple dataset we can perform regression by following these steps:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Separate the outcome variable and the feature variables\n",
        "5. Create a regression model\n",
        "6. Run model on input data and measure error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diamond Prices\n",
        "\n",
        "Let's use the dataset from last week to set up a diamond price estimator.\n",
        "\n",
        "Steps 1 - 3 should look familiar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "DIAMONDS_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/diamonds.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "diamonds_data = object_from_json_url(DIAMONDS_FILE)\n",
        "diamonds_df = pd.DataFrame.from_records(diamonds_data)\n",
        "\n",
        "\n",
        "## 2. Encode non-numeric values\n",
        "cut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
        "color_order = ['J', 'I', 'H', 'G', 'F', 'E', 'D']\n",
        "clarity_order = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n",
        "\n",
        "diamond_encoder = OrdinalEncoder(categories=[cut_order, color_order, clarity_order])\n",
        "\n",
        "ccc_vals = diamond_encoder.fit_transform(diamonds_df[[\"cut\", \"color\", \"clarity\"]].values)\n",
        "diamonds_df[[\"cut\", \"color\", \"clarity\"]] = ccc_vals\n",
        "\n",
        "\n",
        "## 3. Normalize\n",
        "diamond_scaler = MinMaxScaler()\n",
        "diamonds_scaled = diamond_scaler.fit_transform(diamonds_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chose Features\n",
        "\n",
        "Now we separate the outcome variable values and the independent variables.\n",
        "\n",
        "Let's start simple and use only one feature: carat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "carats = diamonds_scaled[[\"carat\"]]\n",
        "\n",
        "# Plot the variables, just for checking\n",
        "plt.scatter(carats, prices, marker='o', linestyle='', alpha=0.3)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "Setup and create regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to their carat value\n",
        "model = price_model.fit(carats, prices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate\n",
        "\n",
        "Run the regression model, put the result back in a DataFrame and measure its error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(carats)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "Hmmm.... what this means is that on average our model is wrong by $\\$1388$ dollars.\n",
        "\n",
        "We can plot our predictions with the original data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the original values\n",
        "plt.scatter(carats, prices, marker='o', linestyle='', alpha=0.3)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "\n",
        "# Plot the predictions\n",
        "plt.scatter(carats, predicted_scaled, color='r', marker='o', linestyle='', alpha=0.05)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "We're only using one variable to model the price using linear regression, so, as the name suggests, the resulting model is a line:\n",
        "\n",
        "$\\displaystyle price = \\beta \\cdot carat$\n",
        "\n",
        "(This should look familiar to a line equation from algebra: $y = m \\cdot x + b$)\n",
        "\n",
        "$\\beta$ in this equation is a constant calculated by the model. The model uses all of the data point values about price and carat to calculate this ONE constant that defines our line.\n",
        "\n",
        "For every value of $carat$ the model gives us a value for $price$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using more features\n",
        "\n",
        "Let's use a few more features to build our model.\n",
        "\n",
        "This time our model equation will have multiple independent variables:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width + \\beta_2 \\cdot length$\n",
        "\n",
        "The $\\beta_i$ values in this equation are constants that the model calculates. There are $3$ now, so the model has more parameters to adjust in order to get a better fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "features = diamonds_scaled[[\"carat\", \"x\", \"y\"]]\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to their carat value as well as width and length\n",
        "price_model.fit(features, prices)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "This is better. The error decreased.\n",
        "\n",
        "We can plot our new model, but since we are limited to $3$ physical dimensions that we can comprehend, we can't plot price as a function of all $3$ of our features.\n",
        "\n",
        "We'll just look at how price varies along with the length and width of the diamonds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "xs = diamonds_df[[\"x\"]].values\n",
        "ys = diamonds_df[[\"y\"]].values\n",
        "\n",
        "ps = diamonds_df[[\"price\"]].values\n",
        "pps = predicted[[\"price\"]].values\n",
        "\n",
        "ax.scatter(xs, ys, ps, marker='o', linestyle='', alpha=0.1)\n",
        "ax.scatter(xs, ys, pps, color='r', marker='o', linestyle='', alpha=0.1)\n",
        "\n",
        "ax.set_xlabel('width')\n",
        "ax.set_ylabel('length')\n",
        "ax.set_zlabel('price')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ALL features\n",
        "\n",
        "Let's use all $9$ features from the dataset to build our model.\n",
        "\n",
        "The model equation is something like:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot x_0 + \\beta_1 \\cdot x_1 + ... + \\beta_8 \\cdot x_8$\n",
        "\n",
        "Where the $x_i$ values are the values of our features (carat, width, etc) and the $\\beta_i$ parameters are the constant values that the model will calculate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "\n",
        "# All except price\n",
        "features = diamonds_scaled.drop(columns=[\"price\"])\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to many features\n",
        "price_model.fit(features, prices)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "The error is getting better.\n",
        "\n",
        "Let's sort and plot all of the prices from the original dataset and the reconstructed prices from our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prices_original = diamonds_df[\"price\"]\n",
        "prices_predicted = predicted[\"price\"]\n",
        "\n",
        "# Plot the original and predicted prices\n",
        "plt.plot(sorted(prices_original), marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(sorted(prices_predicted), color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "The model doesn't look too bad.\n",
        "\n",
        "It seems to not be performing very well for diamonds on the extreme ends of price: the too cheap and too expensive ones.\n",
        "\n",
        "And since even a small percentage of error for an expensive diamond contributes to a large error in dollars, this is probably where a lot of the error is coming from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Even MORE Features !\n",
        "\n",
        "One trick to improve our model is to create some extra features from the current ones.\n",
        "\n",
        "For example, in addition to considering carat and width of each diamond separately, we can create a feature that is a combination of these two values.\n",
        "\n",
        "Considering just those $2$ features, instead of having an equation that is like this:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width$\n",
        "\n",
        "We can try to model an equation that has quadratic terms, like this:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width + \\beta_2 \\cdot carat^2 + \\beta_3 \\cdot width^2 + \\beta_4 \\cdot carat \\cdot width$\n",
        "\n",
        "Or even cubic terms:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width + \\beta_2 \\cdot carat^2 + \\beta_3 \\cdot width^2 + \\beta_4 \\cdot carat \\cdot width + \\beta_5 \\cdot carat^3 + \\beta_6 \\cdot width^3$\n",
        "\n",
        "This allows our model to figure out more complex relationships between the features and consider non-linear relationships between features and price (maybe price goes up proportional to the square of the width of the diamond).\n",
        "\n",
        "SciKit-Learn has an object called [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) that helps us do exactly this. We just have to instantiate it and use it to create some extra features for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "features = diamonds_scaled.drop(columns=[\"price\"])\n",
        "\n",
        "## 4B. Create extra features\n",
        "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
        "features_poly = poly.fit_transform(features)\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to many features\n",
        "result = price_model.fit(features_poly, prices)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features_poly)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "This is significantly better than our original model.\n",
        "\n",
        "Let's sort and plot the resulting prices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot sorted prices\n",
        "prices_original = sorted(diamonds_df[\"price\"])\n",
        "prices_predicted = sorted(predicted[\"price\"])\n",
        "\n",
        "# Plot the original and predicted prices\n",
        "plt.plot(prices_original, marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(prices_predicted, color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More plots\n",
        "\n",
        "And since we can't see in 4D or 5D yet, let's just plot price as a function of a few individual features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot price vs carat, x, y and z\n",
        "for feat in [\"carat\", \"x\", \"y\", \"z\"]:\n",
        "  x = diamonds_df[feat]\n",
        "  prices_original = diamonds_df[\"price\"]\n",
        "  prices_predicted = predicted[\"price\"]\n",
        "\n",
        "  # Plot the original and predicted prices\n",
        "  plt.plot(x, prices_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, prices_predicted, color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"price\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéâüçæ\n",
        "\n",
        "These look ok. We have a nice model for diamond prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Regression!\n",
        "\n",
        "Let's repeat the previous exercise using a different dataset.\n",
        "\n",
        "This one is for wine quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "# Look at features: values, types, names, etc\n",
        "wines_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Normalize\n",
        "wine_scaler = MinMaxScaler()\n",
        "wines_scaled = wine_scaler.fit_transform(wines_df)\n",
        "\n",
        "# Since this is a new dataset, let's just peek at its covariance matrix\n",
        "wines_scaled.cov()[\"quality\"].sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot\n",
        "\n",
        "Looks like alcohol, acidity, density and chlorides are the $4$ features that mostly contribute to the quality.\n",
        "\n",
        "Let's just take a look at graphs of quality as a function of these $4$ features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_df[feat]\n",
        "  quality_original = wines_df[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression\n",
        "\n",
        "Let's just use all of our features to run regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "quality = wines_scaled[\"quality\"]\n",
        "features = wines_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "quality_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates quality of wines to many features\n",
        "result = quality_model.fit(features, quality)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = quality_model.predict(features)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = wine_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Measure error\n",
        "regression_error(wines_df[\"quality\"], predicted[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results\n",
        "\n",
        "On average our predictions are within $0.77$ points of the real quality values.\n",
        "\n",
        "Let's take a look at some plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_df[feat]\n",
        "  quality_original = wines_df[\"quality\"]\n",
        "  quality_predicted = predicted[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, quality_predicted, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "# ü§î\n",
        "\n",
        "Hmm.... these could be better.\n",
        "\n",
        "Our model wasn't able to capture the fact that the resulting quality should be a discrete value and not a number with decimals.\n",
        "\n",
        "This is because our quality category is not continuous, and instead can only have particular discrete values.\n",
        "\n",
        "Instead of trying to calculate continuous values for quality, our model should really be trying to put the wines in the right quality category.\n",
        "\n",
        "Let's use a different type of model for this task.\n",
        "\n",
        "Instead of learning how to predict a continuous value from the independent variables, like this:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-fit.png\" width=\"620px\"/>\n",
        "\n",
        "Our model should learn how to place data points into discrete groups, like this:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-classes.png\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification\n",
        "\n",
        "This is what's called a *classification* problem, or task.\n",
        "\n",
        "Instead of trying to model the behavior of a continuous value, like price or temperature, a classification model tries to predict the correct *label* for given input data.\n",
        "\n",
        "The steps for training a classification model are the same:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Separate the outcome variable and the feature variables\n",
        "5. Create a model\n",
        "6. Run model on input data and test data, and measure error\n",
        "\n",
        "Even though we are trying to predict labels for our data, we still have to encode all label/categorical features, whether they are input or output variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random ? Forest ?\n",
        "\n",
        "The particular classification model we will use is called a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
        "\n",
        "The model gets its name from another type of model called a [Decision Tree](https://scikit-learn.org/stable/modules/tree.html). During training, Decision Trees learn to model our data using simple decision rules, in a process that is conceptually similar to the game [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_questions).\n",
        "\n",
        "It learns to bin our data by *asking* a series of yes/no, if/else, questions using our features:\n",
        "\n",
        "<img src=\"./imgs/decision-trees.jpg\" width=\"720px\"/>\n",
        "\n",
        "A Random Forest Classifier is a model that combines a bunch of Tree models that were trained with randomly selected subsets of our features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "\n",
        "# We're still using scaled feature variables\n",
        "features = wines_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "# But, now our quality variable will be unscaled,\n",
        "# so it keeps its original values as labels 0 - 9\n",
        "quality = wines_df[\"quality\"]\n",
        "\n",
        "## 5. Create a Classifier object\n",
        "quality_model = RandomForestClassifier()\n",
        "\n",
        "# Create a model that classifies quality of wines based on many features\n",
        "result = quality_model.fit(features, quality)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted = quality_model.predict(features)\n",
        "\n",
        "# Measure error\n",
        "regression_error(wines_df[\"quality\"], predicted[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results\n",
        "\n",
        "Ohh, that's better. On average, the model misses the quality value by about $0.1$ points.\n",
        "\n",
        "Let's take a look at some plots to confirm that the model captured the discrete-ness of our label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_df[feat]\n",
        "  quality_original = wines_df[\"quality\"]\n",
        "  quality_predicted = predicted[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, quality_predicted, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Dataset\n",
        "\n",
        "One thing that we didn't do before, but is an important step when training any kind of ML model, is to actually test our model on data that wasn't used in the training. This will tell us whether our model has actually learned patterns from the data or just memorized the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_TEST_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/wines-test.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_test_data = object_from_json_url(WINE_TEST_FILE)\n",
        "wines_test_df = pd.DataFrame.from_records(wines_test_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wines_test_scaled = wine_scaler.transform(wines_test_df)\n",
        "\n",
        "## 4. Separate the independent variables\n",
        "features_test = wines_test_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "## 6. Run the model on the test data\n",
        "predicted_test = quality_model.predict(features_test)\n",
        "\n",
        "# Measure error\n",
        "regression_error(wines_test_df[\"quality\"], predicted_test[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "This is not great, but it's not bad. On average our model is within $0.55$ quality points of the real values.\n",
        "\n",
        "It's better than the result we originally got from linear regression on the test data ($0.76$).\n",
        "\n",
        "Let's take a look at some plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_test_df[feat]\n",
        "  quality_original = wines_test_df[\"quality\"]\n",
        "  quality_predicted = predicted_test[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, quality_predicted, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "We have an *ok* model for predicting wine quality based on a few parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Wine ! üç∑üç∑üç∑\n",
        "\n",
        "Let's pretend we own an online wine store.\n",
        "\n",
        "We already created a model that predicts quality based on a bunch of other properties of wines. We could use this model to figure out how much to pay suppliers for the wine, and how much to charge costumers.\n",
        "\n",
        "But, maybe this \"quality\" feature might not be something we want to share with our costumers. It seems abstract and subjective and would require explanations about our data and our process, which could create confusion.\n",
        "\n",
        "Using the six other features might also not be very useful for costumers who want to buy new wines that are similar to ones that they have previously liked.\n",
        "\n",
        "What we can do is classify the wines into groups that take into account all of the features of the dataset, but present costumers with a more manageable amount of information.\n",
        "\n",
        "This is called [clustering](https://en.wikipedia.org/wiki/Cluster_analysis), or cluster analysis. We'll use it to divide our wines in such a way that wines in the same group, or *cluster*, are more similar to each other than to wines in other clusters.\n",
        "\n",
        "The clusters won't necessarily correlate to the features in our dataset, but will be computed using a combination of the features.\n",
        "\n",
        "Clustering is an example of an *unsupervised* learning method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning\n",
        "\n",
        "The models that we've trained so far for doing regression and classification are considered *supervised* models. During training we give the model our input features, but also provide it with the *correct* values for the output features. These output features tend to be human-labeled values, and are sometimes called the *supervisory signals*.\n",
        "\n",
        "When fully-labeled training data is processed during training, we are hoping that the model learns to extrapolate what it *sees* in the labeled data to new, unseen, unlabeled instances of data with the same input features, but unknown output values.\n",
        "\n",
        "#### Supervised Classification:\n",
        "<img src=\"./imgs/classification-00.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "Unlike supervised learning, unsupervised models learn patterns from unlabeled data. This means all of the features are considered input features, and there are no separate output features or signals. The idea is that by analyzing and processing data in specific ways, the model is able to build a concise representation of its features and create new ways of interpreting, visualizing or generating similar data.\n",
        "\n",
        "We can use unsupervised learning models to explore new datasets and try to simplify our data before we do any kind of supervised learning.\n",
        "\n",
        "The steps for training an unsupervised model should seem familiar:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Select variables and features to be considered\n",
        "5. Create a model\n",
        "6. Run model on input data and test data, and measure error\n",
        "\n",
        "Even though it looks familiar, the last step isn't very obvious. How do we measure error on a model that doesn't have a set of correct answers?\n",
        "\n",
        "Maybe *error* is not the right term, but we'll see how to define *metrics* to score and measure our unsupervised models.\n",
        "\n",
        "#### Unsupervised Clusterings:\n",
        "Since there are no correct labels, both of the following clusterings are valid!\n",
        "\n",
        "<img src=\"./imgs/clustering-00.jpg\" width=\"620px\"/>\n",
        "\n",
        "<img src=\"./imgs/clustering-01.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wine_scaler = StandardScaler()\n",
        "wines_scaled = wine_scaler.fit_transform(wines_df)\n",
        "\n",
        "## 4. Select variables to be considered\n",
        "##    We're gonna drop the quality features to avoid re-clustering by quality\n",
        "features = wines_scaled.drop(columns=[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusterings\n",
        "\n",
        "We are going to look at three different methods for clustering our data.\n",
        "\n",
        "#### [K-means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means):\n",
        "Tries to separate the data into $k$ groups with similar statistical properties. Requires the number of clusters to be determined beforehand, and the algorithm tries to minimize the difference between objects in a cluster.\n",
        "\n",
        "#### [Gaussian Clustering](https://scikit-learn.org/stable/modules/mixture.html#mixture):\n",
        "This is similar to K-means, but this model assumes that all features of our data can be modeled as Gaussian distributions.\n",
        "\n",
        "#### [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering):\n",
        "When appropriate, this method automatically combines and removes a few of our features, before doing K-means clustering. This should always be as good as, or better than, regular K-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = {}\n",
        "\n",
        "## 5. Create Clustering objects\n",
        "n_clusters = 4\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "gm_model = GaussianClustering(n_clusters=n_clusters)\n",
        "sc_model = SpectralClustering(n_clusters=n_clusters)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted['kmeans'] = km_model.fit_predict(features)\n",
        "predicted['gaussian'] = gm_model.fit_predict(features)\n",
        "predicted['spectral'] = sc_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots\n",
        "\n",
        "Since we can't see in $4D$ or $5D$ yet, let's pick $2$ or $3$ variables to visualize our clusters against.\n",
        "\n",
        "We can look at *covariances* again and pick the top $2$ or $3$ variables related to `quality`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Look at covariances again\n",
        "wines_scaled.cov()[\"quality\"].sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "for k,v in predicted.items():\n",
        "  plt.scatter(x, z, c=v[\"clusters\"], marker='o', linestyle='', alpha=0.5)\n",
        "  plt.title(k)\n",
        "  plt.xlabel(xl)\n",
        "  plt.ylabel(zl)\n",
        "  plt.xlim(-2.2, 3.2)\n",
        "  plt.ylim(-2.5, 3.5)\n",
        "  plt.show()\n",
        "\n",
        "for k,v in predicted.items():\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "  ax.scatter(x, y, z, c=v, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "  ax.set_title(k)\n",
        "  ax.set_xlabel(xl)\n",
        "  ax.set_ylabel(yl)\n",
        "  ax.set_zlabel(zl)\n",
        "\n",
        "  ax.set_ylim(-2.5, 8)\n",
        "  ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring\n",
        "\n",
        "Would be nice to have a way to measure how good the clusters are.\n",
        "\n",
        "It would help determine if we need more clusters, or if one method is actually better than the others.\n",
        "\n",
        "There are a couple of ways to do this. We'll look at two of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance\n",
        "\n",
        "The first kind of scoring uses the distances between each point and its cluster's center as a metric.\n",
        "\n",
        "This is sometimes called the L2-distance, and it's just like the more familiar [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) from geometry, but extended to measure more than just $2$ or $3$ dimensions/parameters.\n",
        "\n",
        "Smaller values mean that the cluster center is a good representation of its members."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.distance_error(), gm_model.distance_error(), sc_model.distance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood\n",
        "\n",
        "The second kind of scoring treats each cluster as a potential normal distribution and then calculates the likelihood that each point came from its cluster distribution.\n",
        "\n",
        "Values closer to zero mean that the clusters' statistical properties (mean, variation) are good estimators for the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.likelihood_error(), gm_model.likelihood_error(), sc_model.likelihood_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balance\n",
        "\n",
        "A final metric we can consider when analyzing different clustering algorithms and strategies is to see how balanced the resulting clusters are.\n",
        "\n",
        "This might not be very important in some cases, but in other cases where we might be trying to distribute something equally among a population, this is a good metric to look at.\n",
        "\n",
        "We can compute it by seeing how far our cluster counts are from a balanced clustering with equal size clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we have a list of clusters, like this:\n",
        "print(predicted['kmeans'][:10])\n",
        "\n",
        "# This gives us the counts for each label:\n",
        "label_counts = predicted['kmeans']['clusters'].value_counts()\n",
        "print(label_counts)\n",
        "\n",
        "# And this gives an idea of how far they are from a fully-balanced clustering\n",
        "(label_counts / len(predicted['kmeans']['clusters']) - (1 / n_clusters)).abs().max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balance Error\n",
        "\n",
        "Luckily this has been implemented for us and we can get our clustering model balance error like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.balance_error(), gm_model.balance_error(), sc_model.balance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of clusters\n",
        "\n",
        "By these metrics, it seems like the Spectral Clustering algorithm performs a bit better, even though it doesn't produce the most balanced clusters.\n",
        "\n",
        "Let's try different cluster numbers and see if there's a *better* number of clusters we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_clusters = list(range(1,8))\n",
        "\n",
        "dist_err = []\n",
        "like_err = []\n",
        "bala_err = []\n",
        "\n",
        "for n in num_clusters:\n",
        "  mm = SpectralClustering(n_clusters=n)\n",
        "  mm.fit_predict(features)\n",
        "  dist_err.append(mm.distance_error())\n",
        "  like_err.append(mm.likelihood_error())\n",
        "  bala_err.append(mm.balance_error())\n",
        "\n",
        "\n",
        "plt.plot(num_clusters, dist_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distance Error\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, like_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Likelihood Error\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, bala_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Balance Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Looks like $4$ could be good number of clusters for this model.\n",
        "\n",
        "Let's look at some graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = {}\n",
        "\n",
        "for n in [3,4,5]:\n",
        "  m_model = SpectralClustering(n_clusters=n)\n",
        "  predicted[\"Spectral %s\" % n] = m_model.fit_predict(features)\n",
        "\n",
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "for k,v in predicted.items():\n",
        "  plt.scatter(x, z, c=v[\"clusters\"], marker='o', linestyle='', alpha=0.5)\n",
        "  plt.title(k)\n",
        "  plt.xlabel(xl)\n",
        "  plt.ylabel(zl)\n",
        "  plt.xlim(-2.2, 3.2)\n",
        "  plt.ylim(-2.5, 3.5)\n",
        "  plt.show()\n",
        "\n",
        "for k,v in predicted.items():\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "  ax.scatter(x, y, z, c=v, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "  ax.set_title(k)\n",
        "  ax.set_xlabel(xl)\n",
        "  ax.set_ylabel(yl)\n",
        "  ax.set_zlabel(zl)\n",
        "\n",
        "  ax.set_ylim(-2.5, 8)\n",
        "  ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
